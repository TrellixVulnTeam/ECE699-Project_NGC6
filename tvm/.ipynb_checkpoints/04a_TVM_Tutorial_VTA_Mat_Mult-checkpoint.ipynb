{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeiRi-zc0NuZ"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/04a_TVM_Tutorial_VTA_Mat_Mult.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iERY0FsET3G"
   },
   "source": [
    "Please run the following block to ensure TVM is setup for this notebook, each notebook may have its own runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJkh8uAsET3G"
   },
   "source": [
    "\n",
    "\n",
    "Simple Matrix Multiply\n",
    "======================\n",
    "**Author**: [Thierry Moreau](https://homes.cs.washington.edu/~moreau/)\n",
    "\n",
    "In this tutorial, we will build on top of the `vta-get-started` tutorial\n",
    "and introduce additional concepts required to implement matrix multiplication\n",
    "on VTA with the TVM workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76dzY7NnET3H"
   },
   "source": [
    "RPC Setup\n",
    "---------\n",
    "We start by programming the Pynq's FPGA and building its RPC runtime\n",
    "as we did in the VTA introductory tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XcaHa9QPET3H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized module tvm.runtime._ffi_api\n",
      "initialized module tvm.runtime._ffi_node_api\n",
      "initialized module tvm.runtime.profiling\n",
      "initialized module tvm.ir._ffi_api\n",
      "initialized module tvm.ir._ffi_transform_api\n",
      "initialized module tvm.ir.diagnostics._ffi_api\n",
      "Registering function:diagnostics.override_renderer\n",
      "initialized module tvm.tir._ffi_api\n",
      "Registering function:tvm.default_trace_action\n",
      "initialized module tvm.tir.schedule._ffi_api_schedule\n",
      "initialized module tvm.tir.transform._ffi_api\n",
      "initialized module tvm.tir.analysis._ffi_api\n",
      "initialized module tvm.target._ffi_api\n",
      "Registering function:target._load_config_dict\n",
      "initialized module tvm.te._ffi_api\n",
      "initialized module tvm.te.schedule\n",
      "initialized module tvm.arith._ffi_api\n",
      "initialized module tvm.te.hybrid\n",
      "initialized module tvm.parser._ffi_api\n",
      "initialized module tvm.support\n",
      "Registering function:tvm_callback_rocm_link\n",
      "Registering function:tvm_callback_rocm_bitcode_path\n",
      "Registering function:tvm_callback_libdevice_path\n",
      "Registering function:tvm_callback_sdaccel_compile\n",
      "initialized module tvm.rpc._ffi_api\n",
      "Registering function:rpc.test.addone\n",
      "Registering function:rpc.test.strcat\n",
      "Registering function:rpc.test.except\n",
      "Registering function:rpc.test.runtime_str_concat\n",
      "Registering function:rpc.test.remote_array_func\n",
      "Registering function:rpc.test.add_to_lhs\n",
      "Registering function:rpc.test.remote_return_nd\n",
      "Registering function:rpc.PopenSession\n",
      "Registering function:tvm_callback_cuda_compile\n",
      "Registering function:tvm.info.mem.local.inp_buffer\n",
      "Registering function:tvm.info.mem.local.wgt_buffer\n",
      "Registering function:tvm.info.mem.local.acc_buffer\n",
      "initialized module tvm.topi.cpp\n",
      "initialized module tvm.topi.cpp.cuda\n",
      "initialized module tvm.topi.cpp.nn\n",
      "initialized module tvm.topi.cpp.vision.yolo\n",
      "initialized module tvm.topi.cpp.vision\n",
      "initialized module tvm.topi.cpp.x86\n",
      "initialized module tvm.topi.cpp.generic\n",
      "initialized module tvm.topi.cpp.rocm\n",
      "initialized module tvm.topi.cpp.utils\n",
      "initialized module tvm.auto_scheduler._ffi_api\n",
      "Registering function:auto_scheduler.workload_key_to_tensors\n",
      "Registering function:auto_scheduler.cost_model.random_fill_float\n",
      "Registering function:auto_scheduler.local_builder.build\n",
      "Registering function:auto_scheduler.local_runner.run\n",
      "Registering function:auto_scheduler.rpc_runner.run\n",
      "Registering function:auto_scheduler.enter_layout_rewrite\n",
      "Registering function:auto_scheduler.exit_layout_rewrite\n",
      "Registering function:auto_scheduler.relay_integration.auto_schedule_topi_compute\n",
      "Registering function:tvm.relay.std_path\n",
      "initialized module tvm.relay._ffi_api\n",
      "initialized module tvm.relay.transform._ffi_api\n",
      "initialized module tvm.relay._build_module\n",
      "Registering function:relay.backend.lower\n",
      "Registering function:relay.backend.build\n",
      "Registering function:relay._tensor_value_repr\n",
      "Registering function:relay._constant_repr\n",
      "initialized module tvm.relay.backend._backend\n",
      "Registering function:relay.backend.lower_call\n",
      "initialized module tvm.relay._make\n",
      "initialized module tvm.relay.backend._vm\n",
      "Registering function:tvm.relay.module_export_library\n",
      "Registering function:tvm.relay.build\n",
      "initialized module tvm.relay.analysis._ffi_api\n",
      "initialized module tvm.relay.op._make\n",
      "Registering function:relay.op.compiler._lower\n",
      "Registering function:relay.op.compiler._build\n",
      "initialized module tvm.relay.op.op\n",
      "initialized module tvm.relay.op.dyn._make\n",
      "initialized module tvm.relay.op.vm._ffi_api\n",
      "initialized module tvm.relay.op.dyn.nn._make\n",
      "initialized module tvm.relay.op.nn._make\n",
      "initialized module tvm.relay.op.annotation._make\n",
      "initialized module tvm.relay.op.memory._make\n",
      "initialized module tvm.relay.op.image._make\n",
      "initialized module tvm.relay.op.dyn.image._make\n",
      "initialized module tvm.relay.op.vision._make\n",
      "initialized module tvm.relay.op.random._make\n",
      "Registering function:relay.debug\n",
      "Registering function:relay.debug_interp\n",
      "initialized module tvm.relay.dataflow_pattern._ffi\n",
      "Registering function:relay.ext.arm_compute_lib.optimize\n",
      "Registering function:relay.ext.coremlcompiler\n",
      "initialized module tvm.relay.qnn.op._make\n",
      "initialized module tvm.relay.op.contrib._ethosn\n",
      "initialized module tvm.relay.quantize._quantize\n",
      "Registering function:relay.quantize.attach_simulated_quantize\n",
      "Registering function:relay.transform.MemoryPlan\n",
      "Registering function:relay.transform.LiftConstants\n",
      "initialized module tvm.contrib.nnpack\n",
      "de10pro\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import tvm\n",
    "from tvm import te\n",
    "import vta\n",
    "import numpy as np\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils\n",
    "from vta.testing import simulator\n",
    "\n",
    "# Load VTA parameters from the vta/config/vta_config.json file\n",
    "env = vta.get_env()\n",
    "\n",
    "# Print the target specified in the vta_config.json\n",
    "print(env.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2xgTgtmnET3H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVMModGetFunction:tvm.contrib.vta.reconfig_runtime\n",
      "TVMModGetFunction:tvm.contrib.vta.init\n",
      "TVMModGetFunction:tvm.rpc.server.upload\n",
      "<tvm.rpc.client.RPCSession object at 0x7f88e069d700>\n"
     ]
    }
   ],
   "source": [
    "# We read the Pynq RPC host IP address and port number from the OS environment\n",
    "host = os.environ.get(\"VTA_RPC_HOST\", \"192.168.101.199\")\n",
    "port = int(os.environ.get(\"VTA_RPC_PORT\", \"9091\"))\n",
    "\n",
    "# We configure both the bitstream and the runtime system on the Pynq\n",
    "# to match the VTA configuration specified by the vta_config.json file.\n",
    "if env.TARGET == \"de10pro\":\n",
    "\n",
    "    # Make sure that TVM was compiled with RPC=1\n",
    "    assert tvm.runtime.enabled(\"rpc\")\n",
    "    remote = rpc.connect(host, port)\n",
    "\n",
    "    # Reconfigure the JIT runtime\n",
    "    vta.reconfig_runtime(remote)\n",
    "\n",
    "    # Program the FPGA with a pre-compiled VTA bitstream.\n",
    "    # You can program the FPGA with your own custom bitstream\n",
    "    # by passing the path to the bitstream file instead of None.\n",
    "    vta.program_fpga(remote, bitstream=None)\n",
    "\n",
    "# In simulation mode, host the RPC server locally.\n",
    "elif env.TARGET == \"sim\":\n",
    "    remote = rpc.LocalSession()\n",
    "\n",
    "print(remote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K0FHug0ET3H"
   },
   "source": [
    "Computation Declaration\n",
    "-----------------------\n",
    "In this example we describe a simple matrix multiplication addition, which\n",
    "requires multiple computation stages, as shown in the dataflow diagram below.\n",
    "First we describe the input tensors `A` and `B` that are living\n",
    "in main memory.\n",
    "Second, we need to declare intermediate tensors `A_buf` and\n",
    "`B_buf`, which will live in VTA's on-chip buffers.\n",
    "Having this extra computational stage allows us to explicitly\n",
    "stage cached reads and writes.\n",
    "Third, we describe the matrix multiplication computation over\n",
    "`A_buf` and `B_buf` to produce the product matrix `C_buf`.\n",
    "The last operation is a cast and copy back to DRAM, into results tensor\n",
    "`C`.\n",
    "\n",
    "![](https://raw.githubusercontent.com/uwsaml/web-data/master/vta/tutorial/gemm_dataflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVXuAgb9ET3I"
   },
   "source": [
    "### Data Layout\n",
    "We describe the placeholder tensors `A`, and `B` in a tiled data\n",
    "format to match the data layout requirements imposed by the VTA tensor core.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l04CJvJqET3I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are 16 by 16, of type int8\n",
      "Input activations are 1 by 16, of type int8\n",
      "Output activations are 1 by 16, of type int32\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the specific dimensions of our GEMM core\n",
    "print(\"Weights are {} by {}, of type {}\".format(\n",
    "    env.BLOCK_OUT, env.BLOCK_IN, env.wgt_dtype))\n",
    "print(\"Input activations are {} by {}, of type {}\".format(\n",
    "    env.BATCH, env.BLOCK_IN, env.inp_dtype))\n",
    "print(\"Output activations are {} by {}, of type {}\".format(\n",
    "    env.BATCH, env.BLOCK_OUT, env.acc_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TAeVMW48ET3I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight buffer requires 2048 bits per cycle of read xput\n",
      "Input buffer requires 128 bits per cycle of read xput\n",
      "Register file requires 512 bits per cycle of read & write xput\n"
     ]
    }
   ],
   "source": [
    "# We can also derive throughput requirements for each memory\n",
    "print(\"Weight buffer requires {} bits per cycle of read xput\".format(\n",
    "    env.BLOCK_OUT * env.BLOCK_IN * env.WGT_WIDTH))\n",
    "print(\"Input buffer requires {} bits per cycle of read xput\".format(\n",
    "    env.BATCH * env.BLOCK_IN * env.INP_WIDTH))\n",
    "print(\"Register file requires {} bits per cycle of read & write xput\".format(\n",
    "    env.BATCH * env.BLOCK_OUT * env.ACC_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IppFJoiEET3I"
   },
   "outputs": [],
   "source": [
    "# Output channel factor m - total 4=256 output channels\n",
    "m = 4\n",
    "# Input channel factor n - total 4=256 input channels\n",
    "n = 4\n",
    "# Batch factor o (we use single batch inference)\n",
    "o = 1\n",
    "# A placeholder tensor in tiled data format\n",
    "A = te.placeholder((o, n, env.BATCH, env.BLOCK_IN),\n",
    "                   name=\"A\", dtype=env.inp_dtype)\n",
    "# B placeholder tensor in tiled data format\n",
    "B = te.placeholder((m, n, env.BLOCK_OUT, env.BLOCK_IN),\n",
    "                   name=\"B\", dtype=env.wgt_dtype)\n",
    "# A copy buffer\n",
    "A_buf = te.compute((o, n, env.BATCH, env.BLOCK_IN),\n",
    "                   lambda *i: A(*i), \"A_buf\")\n",
    "# B copy buffer\n",
    "B_buf = te.compute((m, n, env.BLOCK_OUT, env.BLOCK_IN),\n",
    "                   lambda *i: B(*i), \"B_buf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yqzo7JpET3J"
   },
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "Now we're ready to describe the matrix multiplication result tensor `C`,\n",
    "with another compute operation.\n",
    "The compute function takes the shape of the tensor, as well as a lambda\n",
    "function that describes the computation rule for each position of the tensor.\n",
    "\n",
    "No computation happens during this phase, as we are only declaring how\n",
    "the computation should be done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cYwjRPZbET3J"
   },
   "outputs": [],
   "source": [
    "# Outer input feature reduction axis\n",
    "ko = te.reduce_axis((0, n), name=\"ko\")\n",
    "# Inner input feature reduction axis\n",
    "ki = te.reduce_axis((0, env.BLOCK_IN), name=\"ki\")\n",
    "# Describe the in-VTA matrix multiplication\n",
    "C_buf = te.compute(\n",
    "    (o, m, env.BATCH, env.BLOCK_OUT),\n",
    "    lambda bo, co, bi, ci:\n",
    "        te.sum(A_buf[bo, ko, bi, ki].astype(env.acc_dtype) *\n",
    "                B_buf[co, ko, ci, ki].astype(env.acc_dtype),\n",
    "                axis=[ko, ki]),\n",
    "    name=\"C_buf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX4w4YKfET3J"
   },
   "source": [
    "### Casting the Results\n",
    "\n",
    "After the computation is done, we'll need to send the results computed by VTA back to main memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-ptVKM_lET3J"
   },
   "outputs": [],
   "source": [
    "# Cast to output type, and send to main memory\n",
    "C = te.compute(\n",
    "    (o, m, env.BATCH, env.BLOCK_OUT),\n",
    "    lambda *i: C_buf(*i).astype(env.inp_dtype),\n",
    "    name=\"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqDEtNdcET3J"
   },
   "source": [
    "This concludes the computation declaration part of this tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySK9O86jET3K"
   },
   "source": [
    "Scheduling the Computation\n",
    "--------------------------\n",
    "While the above lines describes the computation rule, we can obtain\n",
    "`C` in many ways.\n",
    "TVM asks the user to provide an implementation of the computation called\n",
    "*schedule*.\n",
    "\n",
    "A schedule is a set of transformations to an original computation that\n",
    "transforms the implementation of the computation without affecting\n",
    "correctness.\n",
    "This simple VTA programming tutorial aims to demonstrate basic schedule\n",
    "transformations that will map the original schedule down to VTA hardware\n",
    "primitives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0HRArhlET3K"
   },
   "source": [
    "### Default Schedule\n",
    "\n",
    "After we construct the schedule, by default the schedule computes\n",
    "`C` in the following way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fDKGqneGET3K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(int8), int8, [1, 1, 1, 16], []),\n",
      "             C: Buffer(C_2: Pointer(int8), int8, [1, 1, 1, 16], []),\n",
      "             B: Buffer(B_2: Pointer(int8), int8, [1, 1, 16, 16], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  attr [A_buf: Pointer(int8)] \"storage_scope\" = \"global\";\n",
      "  allocate(A_buf, int8, [16]);\n",
      "  attr [B_buf: Pointer(int8)] \"storage_scope\" = \"global\";\n",
      "  allocate(B_buf, int8, [256]);\n",
      "  attr [C_buf: Pointer(int32)] \"storage_scope\" = \"global\";\n",
      "  allocate(C_buf, int32, [16]) {\n",
      "    for (i3: int32, 0, 16) {\n",
      "      A_buf[i3] = (int8*)A_2[i3]\n",
      "    }\n",
      "    for (i2: int32, 0, 16) {\n",
      "      for (i3_1: int32, 0, 16) {\n",
      "        B_buf[((i2*16) + i3_1)] = (int8*)B_2[((i2*16) + i3_1)]\n",
      "      }\n",
      "    }\n",
      "    for (ci: int32, 0, 16) {\n",
      "      C_buf[ci] = 0\n",
      "      for (ki: int32, 0, 16) {\n",
      "        C_buf[ci] = ((int32*)C_buf[ci] + (cast(int32, (int8*)A_buf[ki])*cast(int32, (int8*)B_buf[((ci*16) + ki)])))\n",
      "      }\n",
      "    }\n",
      "    for (i3_2: int32, 0, 16) {\n",
      "      C_2[i3_2] = cast(int8, (int32*)C_buf[i3_2])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the generated schedule\n",
    "s = te.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Rm-Yn7ET3K"
   },
   "source": [
    "Although this schedule makes sense, it won't compile to VTA.\n",
    "In order to obtain correct code generation, we need to apply scheduling\n",
    "primitives and code annotation that will transform the schedule into\n",
    "one that can be directly lowered onto VTA hardware intrinsics.\n",
    "Those include:\n",
    "\n",
    " - DMA copy operations which will take globally-scoped tensors and copy\n",
    "   those into locally-scoped tensors.\n",
    " - Tensor operations that will perform the matrix multiplication.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYb-FFZaET3K"
   },
   "source": [
    "### Buffer Scopes\n",
    "\n",
    "First, we set the scope of the buffers to tell TVM that these buffers\n",
    "will be living in the VTA's on-chip SRAM caches.\n",
    "Below, we tell TVM that `A_buf`, `B_buf`, `C_buf`\n",
    "will respectively live in VTA's on-chip input, weight and accumulator\n",
    "memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN0F0S8vET3K"
   },
   "source": [
    "<div class=\"alert alert-info\"><h4>VTA's On-Chip SRAMs</h4><p>\n",
    "\n",
    "  VTA has three different memory scopes, each corresponding to different\n",
    "  on-chip SRAM buffers.\n",
    "\n",
    "   - `env.inp_scope`: Input buffer, which is a read-only SRAM buffer\n",
    "     that stores activation tensors.\n",
    "   - `env.wgt_scope`: Weight buffer, which is a read-only SRAM buffer\n",
    "     that stores weight tensors.\n",
    "   - `env.acc_scope`: Register file for accumulation, which is a\n",
    "      read/write SRAM buffer that stores accumulator tensors.</p></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mKi6MQLbET3L"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stage(C_buf, compute(C_buf, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0]), source=[(int32(A_buf[bo, ko, bi, ki])*int32(B_buf[co, ko, ci, ki]))], init=[], axis=[iter_var(ko, range(min=0, ext=1)), iter_var(ki, range(min=0, ext=16))], where=(bool)1, value_index=0)], axis=[iter_var(bo, range(min=0, ext=1)), iter_var(co, range(min=0, ext=1)), iter_var(bi, range(min=0, ext=1)), iter_var(ci, range(min=0, ext=16))], reduce_axis=[iter_var(ko, range(min=0, ext=1)), iter_var(ki, range(min=0, ext=16))], tag=, attrs={}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the intermediate tensor's scope to VTA's on-chip buffers\n",
    "s[A_buf].set_scope(env.inp_scope)\n",
    "s[B_buf].set_scope(env.wgt_scope)\n",
    "s[C_buf].set_scope(env.acc_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATmO6AIcET3L"
   },
   "source": [
    "### DMA Transfers\n",
    "\n",
    "We need to schedule DMA transfers to move data living in DRAM to\n",
    "and from the VTA on-chip buffers.\n",
    "This can be achieved using the `compute_at` schedule primitive\n",
    "which nests the copying of the buffers into the computation loop\n",
    "that performs the matrix multiplication.\n",
    "\n",
    "We insert `dma_copy` pragmas to indicate to the compiler\n",
    "that the copy operations will be performed in bulk via DMA,\n",
    "which is common in hardware accelerators.\n",
    "Finally, we print the temporary schedule to observe the effects of\n",
    "moving the copy operations into the matrix multiplication loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dS3h74uaET3L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {C: Buffer(C_2: Pointer(int8), int8, [1, 1, 1, 16], []),\n",
      "             A: Buffer(A_2: Pointer(int8), int8, [1, 1, 1, 16], []),\n",
      "             B: Buffer(B_2: Pointer(int8), int8, [1, 1, 16, 16], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  attr [C_buf: Pointer(int32)] \"storage_scope\" = \"local.acc_buffer\";\n",
      "  allocate(C_buf, int32, [16]);\n",
      "  attr [A_buf: Pointer(int8)] \"storage_scope\" = \"local.inp_buffer\";\n",
      "  allocate(A_buf, int8, [16]);\n",
      "  attr [B_buf: Pointer(int8)] \"storage_scope\" = \"local.wgt_buffer\";\n",
      "  allocate(B_buf, int8, [16]) {\n",
      "    for (ci: int32, 0, 16) {\n",
      "      C_buf[ci] = 0\n",
      "      attr [IterVar(i0: int32, (nullptr), \"DataPar\", \"\")] \"pragma_dma_copy\" = 1;\n",
      "      for (i3: int32, 0, 16) {\n",
      "        A_buf[i3] = (int8*)A_2[i3]\n",
      "      }\n",
      "      attr [IterVar(i0_1: int32, (nullptr), \"DataPar\", \"\")] \"pragma_dma_copy\" = 1;\n",
      "      for (i3_1: int32, 0, 16) {\n",
      "        B_buf[i3_1] = (int8*)B_2[((ci*16) + i3_1)]\n",
      "      }\n",
      "      for (ki: int32, 0, 16) {\n",
      "        C_buf[ci] = ((int32*)C_buf[ci] + (cast(int32, (int8*)A_buf[ki])*cast(int32, (int8*)B_buf[ki])))\n",
      "      }\n",
      "    }\n",
      "    attr [IterVar(i0_2: int32, (nullptr), \"DataPar\", \"\")] \"pragma_dma_copy\" = 1;\n",
      "    for (i3_2: int32, 0, 16) {\n",
      "      C_2[i3_2] = cast(int8, (int32*)C_buf[i3_2])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Move buffer copy into matrix multiply loop\n",
    "s[A_buf].compute_at(s[C_buf], ko)\n",
    "s[B_buf].compute_at(s[C_buf], ko)\n",
    "\n",
    "# Tag the buffer copies with the DMA pragma to insert a DMA transfer\n",
    "s[A_buf].pragma(s[A_buf].op.axis[0], env.dma_copy)\n",
    "s[B_buf].pragma(s[B_buf].op.axis[0], env.dma_copy)\n",
    "s[C].pragma(s[C].op.axis[0], env.dma_copy)\n",
    "\n",
    "# Let's take a look at the transformed schedule\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKFZKUf6ET3L"
   },
   "source": [
    "### Tensorization\n",
    "\n",
    "The last step of the schedule transformation consists in applying\n",
    "*tensorization* to our schedule.\n",
    "Tensorization is analogous to vectorization, but extends the concept\n",
    "to a higher-dimensional unit of computation.\n",
    "Consequently, tensorization imposes data layout constraints as discussed\n",
    "earlier when declaring the data layout input placeholders.\n",
    "We've already arranged our tensors in a tiled format, so the next thing\n",
    "we need to perform is loop reordering to accommodate for tensorization.\n",
    "\n",
    "Here we choose to move the outermost reduction axis all the way out.\n",
    "This dictates that we first iterate over input channels, then batch\n",
    "dimensions, and finally output channels.\n",
    "Lastly, we apply the tensorization scheduling primitive `tensorize`\n",
    "along the outer axis of the inner-most matrix matrix multiplication tensor\n",
    "block.\n",
    "We print the finalized schedule that is ready for code-generation\n",
    "by the VTA runtime JIT compiler.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T7mEqzDWET3L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(int8), int8, [1, 1, 1, 16], []),\n",
      "             C: Buffer(C_2: Pointer(int8), int8, [1, 1, 1, 16], []),\n",
      "             B: Buffer(B_2: Pointer(int8), int8, [1, 1, 16, 16], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  attr [C_buf: Pointer(int32)] \"storage_scope\" = \"local.acc_buffer\";\n",
      "  attr [A_buf: Pointer(int8)] \"storage_scope\" = \"local.inp_buffer\";\n",
      "  attr [B_buf: Pointer(int8)] \"storage_scope\" = \"local.wgt_buffer\" {\n",
      "    attr [IterVar(vta: int32, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushGEMMOp\";\n",
      "      @tir.vta.uop_push(0, 1, 0, 0, 0, 0, 0, 0, dtype=int32)\n",
      "      @tir.vta.coproc_dep_push(2, 1, dtype=int32)\n",
      "    }\n",
      "    attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 1 {\n",
      "      @tir.vta.coproc_dep_pop(2, 1, dtype=int32)\n",
      "      @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), A_2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, dtype=int32)\n",
      "      @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), B_2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, dtype=int32)\n",
      "      @tir.vta.coproc_dep_push(1, 2, dtype=int32)\n",
      "    }\n",
      "    attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "      @tir.vta.coproc_dep_pop(1, 2, dtype=int32)\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushGEMMOp\";\n",
      "      @tir.vta.uop_push(0, 0, 0, 0, 0, 0, 0, 0, dtype=int32)\n",
      "      @tir.vta.coproc_dep_push(2, 3, dtype=int32)\n",
      "    }\n",
      "    attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 3 {\n",
      "      @tir.vta.coproc_dep_pop(2, 3, dtype=int32)\n",
      "      @tir.call_extern(\"VTAStoreBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), 0, 4, C_2, 0, 1, 1, 1, dtype=int32)\n",
      "    }\n",
      "    @tir.vta.coproc_sync(, dtype=int32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s[C_buf].reorder(\n",
    "    ko,\n",
    "    s[C_buf].op.axis[0],\n",
    "    s[C_buf].op.axis[1],\n",
    "    s[C_buf].op.axis[2],\n",
    "    s[C_buf].op.axis[3],\n",
    "    ki)\n",
    "s[C_buf].tensorize(s[C_buf].op.axis[2], env.gemm)\n",
    "\n",
    "# Let's take a look at the finalized schedule\n",
    "print(vta.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXf8oKyvET3L"
   },
   "source": [
    "This concludes the scheduling portion of this tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvPKWgYfET3M"
   },
   "source": [
    "TVM Compilation\n",
    "---------------\n",
    "After we have finished specifying the schedule, we can compile it\n",
    "into a TVM function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "N754Lhr2ET3M"
   },
   "outputs": [],
   "source": [
    "# Build GEMM VTA kernel (set debug flags)\n",
    "with vta.build_config(debug_flag = 0x6):\n",
    "    my_gemm = tvm.build(s, [A, B, C], \"ext_dev\",\n",
    "                        env.target_host, name=\"my_gemm\")\n",
    "\n",
    "# Write the compiled module into an object file.\n",
    "temp = utils.tempdir()\n",
    "my_gemm.save(temp.relpath(\"gemm.o\"))\n",
    "\n",
    "# Send the executable over RPC\n",
    "remote.upload(temp.relpath(\"gemm.o\"))\n",
    "\n",
    "# Load the compiled module\n",
    "f = remote.load_module(\"gemm.o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYw4XgOYET3M"
   },
   "source": [
    "Running the Function\n",
    "--------------------\n",
    "The compiled TVM function uses a concise C API and can be invoked from\n",
    "code language.\n",
    "\n",
    "TVM provides an array API in python to aid quick testing and prototyping.\n",
    "The array API is based on [DLPack](https://github.com/dmlc/dlpack) standard.\n",
    "\n",
    "- We first create a remote context (for remote execution on the Pynq).\n",
    "- Then `tvm.nd.array` formats the data accordingly.\n",
    "- `f()` runs the actual computation.\n",
    "- `asnumpy()` copies the result array back in a format that can be\n",
    "  interpreted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GEaLA6kzET3M",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVMModGetFunction:__tvm_main__\n"
     ]
    },
    {
     "ename": "RPCError",
     "evalue": "Traceback (most recent call last):\n  8: TVMFuncCall\n  7: _ZNSt17_Function_handlerIFvN3tvm7runtime7TVMArgsEPNS1_11\n  6: tvm::runtime::RPCWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  5: tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::TVMArgs)> const&)\n  4: tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::TVMArgs)>)\n  3: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::function<void (tvm::runtime::TVMArgs)>)\n  2: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::function<void (tvm::runtime::TVMArgs)>)\n  1: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::function<void (tvm::runtime::TVMArgs)>)\n  0: tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::function<void (tvm::runtime::TVMArgs)>)\n  12: TVMFuncCall\n  11: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  10: tvm::runtime::RPCServerLoop(int)\n  9: tvm::runtime::RPCEndpoint::ServerLoop()\n  8: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::function<void (tvm::runtime::TVMArgs)>)\n  7: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::function<void (tvm::runtime::TVMArgs)>)\n  6: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::function<void (tvm::runtime::TVMArgs)>)\n  5: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)\n  4: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::TVMArgs)> const&)\n  3: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  2: my_gemm\n  1: VTATLSCommandHandle\n  0: vta::BaseQueue<VTAGenericInsn>::InitSpace(unsigned int, unsigned int, bool, bool) [clone .part.113]\n  File \"/media/tliu/ECE699/tvm-repo/tvm/src/runtime/rpc/rpc_endpoint.cc\", line 376\nRPCError: Error caught from RPC call:\n[20:26:49] /home/terasic/workspace/tvm/vta/runtime/runtime.cc:401: Check failed: (fpga_buff_ != nullptr) is false: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRPCError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d0af2f4e5a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Invoke the module to perform the computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_nd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/tliu/ECE699/tvm-repo/tvm/python/tvm/runtime/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/tliu/ECE699/tvm-repo/tvm/python/tvm/_ffi/_ctypes/packed_func.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         ):\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRPCError\u001b[0m: Traceback (most recent call last):\n  8: TVMFuncCall\n  7: _ZNSt17_Function_handlerIFvN3tvm7runtime7TVMArgsEPNS1_11\n  6: tvm::runtime::RPCWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  5: tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::TVMArgs)> const&)\n  4: tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::TVMArgs)>)\n  3: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::function<void (tvm::runtime::TVMArgs)>)\n  2: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::function<void (tvm::runtime::TVMArgs)>)\n  1: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::function<void (tvm::runtime::TVMArgs)>)\n  0: tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::function<void (tvm::runtime::TVMArgs)>)\n  12: TVMFuncCall\n  11: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  10: tvm::runtime::RPCServerLoop(int)\n  9: tvm::runtime::RPCEndpoint::ServerLoop()\n  8: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::function<void (tvm::runtime::TVMArgs)>)\n  7: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::function<void (tvm::runtime::TVMArgs)>)\n  6: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::function<void (tvm::runtime::TVMArgs)>)\n  5: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)\n  4: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::TVMArgs)> const&)\n  3: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  2: my_gemm\n  1: VTATLSCommandHandle\n  0: vta::BaseQueue<VTAGenericInsn>::InitSpace(unsigned int, unsigned int, bool, bool) [clone .part.113]\n  File \"/media/tliu/ECE699/tvm-repo/tvm/src/runtime/rpc/rpc_endpoint.cc\", line 376\nRPCError: Error caught from RPC call:\n[20:26:49] /home/terasic/workspace/tvm/vta/runtime/runtime.cc:401: Check failed: (fpga_buff_ != nullptr) is false: \n"
     ]
    }
   ],
   "source": [
    "# Get the remote device context\n",
    "ctx = remote.ext_dev(0)\n",
    "\n",
    "# Initialize the A and B arrays randomly in the int range of (-128, 128]\n",
    "A_orig = np.random.randint(\n",
    "    -128, 128,\n",
    "    size=(o * env.BATCH, n * env.BLOCK_IN)).astype(A.dtype)\n",
    "B_orig = np.random.randint(\n",
    "    -128, 128,\n",
    "    size=(m * env.BLOCK_OUT, n * env.BLOCK_IN)).astype(B.dtype)\n",
    "\n",
    "# Apply packing to the A and B arrays from a 2D to a 4D packed layout\n",
    "A_packed = A_orig.reshape(\n",
    "    o, env.BATCH, n, env.BLOCK_IN).transpose((0, 2, 1, 3))\n",
    "B_packed = B_orig.reshape(\n",
    "    m, env.BLOCK_OUT, n, env.BLOCK_IN).transpose((0, 2, 1, 3))\n",
    "\n",
    "# Format the input/output arrays with tvm.nd.array to the DLPack standard\n",
    "A_nd = tvm.nd.array(A_packed, ctx)\n",
    "B_nd = tvm.nd.array(B_packed, ctx)\n",
    "C_nd = tvm.nd.array(\n",
    "    np.zeros((o, m, env.BATCH, env.BLOCK_OUT)).astype(C.dtype), ctx)\n",
    "\n",
    "# Invoke the module to perform the computation\n",
    "f(A_nd, B_nd, C_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvH-jJijET3M"
   },
   "source": [
    "Verifying Correctness\n",
    "---------------------\n",
    "Compute the reference result with numpy and assert that the output of the\n",
    "matrix multiplication indeed is correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3kQYkeMET3M"
   },
   "outputs": [],
   "source": [
    "# Compute reference result with numpy\n",
    "C_ref = np.dot(A_orig.astype(env.acc_dtype),\n",
    "               B_orig.T.astype(env.acc_dtype)).astype(C.dtype)\n",
    "C_ref = C_ref.reshape(\n",
    "    o, env.BATCH, m, env.BLOCK_OUT).transpose((0, 2, 1, 3))\n",
    "np.testing.assert_equal(C_ref, C_nd.asnumpy())\n",
    "print(\"Successful matrix multiply test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r0ZH4FMET3M"
   },
   "source": [
    "Summary\n",
    "-------\n",
    "This tutorial showcases the TVM workflow to implement a simple matrix\n",
    "multiplication example on VTA.\n",
    "The general workflow includes:\n",
    "\n",
    "- Programming the FPGA with the VTA bitstream over RPC.\n",
    "- Describing matrix multiplication via a series of computations.\n",
    "- Describing how we want to perform the computation using schedule primitives.\n",
    "- Compiling the function to the VTA target.\n",
    "- Running the compiled module and verifying it against a numpy implementation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "04a_TVM_Tutorial_VTA_Mat_Mult.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
